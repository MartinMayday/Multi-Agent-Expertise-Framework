# =============================================================================
# Context Generator - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# Never commit .env to version control
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Selection
# -----------------------------------------------------------------------------
# Format: provider/model
# Examples:
#   - ollama/llama3.2
#   - lmstudio/local-model
#   - openrouter/anthropic/claude-3-haiku
#   - gemini/gemini-1.5-flash
#   - moonshot/moonshot-v1-8k
#   - offline (no LLM, static analysis only)

# Use provider/model format: provider/modelname
# Examples:
#   CONTEXT_GENERATOR_MODEL=offline                          (static analysis only)
#   CONTEXT_GENERATOR_MODEL=ollama/llama3.2                  (local, recommended for privacy)
#   CONTEXT_GENERATOR_MODEL=openrouter/anthropic/claude-3-haiku (cloud, fastest)
#   CONTEXT_GENERATOR_MODEL=gemini/gemini-1.5-flash          (Google API)
#   CONTEXT_GENERATOR_MODEL=moonshot/moonshot-v1-8k          (China, lower latency)
CONTEXT_GENERATOR_MODEL=offline

# -----------------------------------------------------------------------------
# Ollama (Local)
# -----------------------------------------------------------------------------
# https://ollama.ai
# Default: http://localhost:11434

OLLAMA_HOST=http://localhost:11434

# Available models - Run `ollama list` to see installed models
# 
# FAST/SMALL (good for iterations, local dev):
#   - llama3.2 (3.2B params, balanced)
#   - llama3.2:1b (1B params, fastest)
#   - gemma2:2b (2B params, good for code)
#   - mistral:7b (7B params, good reasoning)
#
# BALANCED (recommended for context generation):
#   - llama3.2 (default 3B variant)
#   - llama3.1:8b (8B params, strong all-around)
#   - neural-chat:7b (7B, tuned for dialogue)
#   - starling-lm:7b (7B, instruction-tuned)
#   - deepseek-coder:6.7b (6.7B, specialized for code)
#   - codellama:7b (7B, code-focused)
#
# POWERFUL (better quality, slower):
#   - llama3.1:70b (70B params, slow but excellent)
#   - mixtral:8x7b (56B total, MoE model)
#   - nous-hermes:34b (34B, strong reasoning)
#
# SPECIALIZED:
#   - qwen2.5-coder:7b (code generation)
#   - deepseek-coder:33b-instruct (code, larger)
#   - phi:2.7b (efficient, small context)
#
# To use: CONTEXT_GENERATOR_MODEL=ollama/llama3.2

# -----------------------------------------------------------------------------
# LM Studio (Local)
# -----------------------------------------------------------------------------
# https://lmstudio.ai
# Default server runs on port 1234, OpenAI-compatible API

#LMSTUDIO_HOST=http://localhost:1234
#LMSTUDIO_API_KEY=lm-studio

# LM Studio Configuration:
# 1. Download and run LM Studio: https://lmstudio.ai
# 2. Load a model in LM Studio UI
# 3. Go to Developer Console (gear icon)
# 4. Start the local server (default port 1234)
# 5. Use the loaded model name in CONTEXT_GENERATOR_MODEL
#
# Example with LM Studio running a model:
#   CONTEXT_GENERATOR_MODEL=lmstudio/TheBloke/Llama-2-7B-Chat-GGUF
#
# LM Studio uses OpenAI-compatible endpoint:
# POST http://localhost:1234/v1/chat/completions

# -----------------------------------------------------------------------------
# OpenRouter (Cloud - Multi-provider)
# -----------------------------------------------------------------------------
# https://openrouter.ai
# Get API key: https://openrouter.ai/keys

OPENROUTER_API_KEY=sk-or-v1-your-api-key-here

# Available Models (multi-provider access):
#
# FASTEST/CHEAPEST:
#   - google/gemini-flash-1.5 (1M context, fastest)
#   - deepseek/deepseek-chat (good code, very cheap)
#   - meta-llama/llama-3.1-8b-instruct (open source, cheap)
#
# BALANCED (recommended):
#   - anthropic/claude-3-haiku (fast, $0.80/1M input)
#   - anthropic/claude-3.5-sonnet (better quality, $3/1M input)
#   - google/gemini-pro (balanced, $1.25/1M input)
#
# POWERFUL (best quality):
#   - anthropic/claude-3-opus (best reasoning, $15/1M input)
#   - deepseek/deepseek-reasoner (long reasoning, moderate cost)
#   - meta-llama/llama-3.1-70b-instruct (open source, large)
#
# To use: CONTEXT_GENERATOR_MODEL=openrouter/anthropic/claude-3-haiku
#
# Features:
# - Access 200+ models from 30+ providers (Claude, GPT, Gemini, Llama, etc.)
# - Single API key, no provider signup needed
# - Usage-based pricing, no monthly fees
# - Fallback between providers

# Optional: Site info for OpenRouter analytics
OPENROUTER_SITE_URL=https://github.com/context-generator
OPENROUTER_SITE_NAME=Context Generator

# -----------------------------------------------------------------------------
# Google Gemini (Cloud)
# -----------------------------------------------------------------------------
# https://ai.google.dev
# Get API key: https://aistudio.google.com/apikey

GEMINI_API_KEY=your-gemini-api-key-here

# Available Models:
#
# FAST/COST-EFFECTIVE:
#   - gemini-2.0-flash (latest, fast)
#   - gemini-1.5-flash (1M context, very fast)
#   - gemini-1.5-flash-8b (lightweight version)
#
# BALANCED:
#   - gemini-1.5-pro (2M context, better reasoning)
#   - gemini-2.0-pro-exp (experimental, advanced capabilities)
#
# To use: CONTEXT_GENERATOR_MODEL=gemini/gemini-1.5-flash
#
# Features:
# - Up to 2M token context window
# - Free tier: 15 requests/min, 1M tokens/day
# - Paid: Usage-based pricing
# - Good for code analysis and documentation generation

# Optional: Custom endpoint
GEMINI_API_BASE=https://generativelanguage.googleapis.com/v1beta

# -----------------------------------------------------------------------------
# Moonshot AI (Cloud - China)
# -----------------------------------------------------------------------------
# https://www.moonshot.cn / https://platform.moonshot.cn
# Get API key: https://platform.moonshot.cn/console/api-keys
# NOTE: Moonshot AI is optimized for users in China

MOONSHOT_API_KEY=sk-your-moonshot-api-key-here

# Available Models (via OpenAI-compatible API):
#
# CONTEXT WINDOW OPTIONS:
#   - moonshot-v1-8k (8K context, standard)
#   - moonshot-v1-32k (32K context, longer documents)
#   - moonshot-v1-128k (128K context, code repositories)
#
# REASONING/PERFORMANCE:
#   - kimi-k2-turbo-preview (faster inference)
#   - kimi-k2-thinking (advanced reasoning, slower)
#
# To use: CONTEXT_GENERATOR_MODEL=moonshot/moonshot-v1-8k
#
# Features:
# - OpenAI-compatible API (easy integration)
# - Lower latency for China-based users
# - Competitive pricing on large context windows
# - Good multilingual support

MOONSHOT_API_BASE=https://api.moonshot.cn/v1

# -----------------------------------------------------------------------------
# ZAI Codingplan (Custom/Enterprise)
# -----------------------------------------------------------------------------
# Configure your ZAI Codingplan endpoint
# https://platform.zai.ai / https://console.zai.ai

ZAI_API_KEY=sk-your-zai-api-key-here
ZAI_API_BASE=https://api.zai.ai/v1
ZAI_MODEL=gpt-4-turbo

# Available Models (check your ZAI console for available models):
#   - gpt-4-turbo (advanced reasoning)
#   - gpt-4 (balanced)
#   - gpt-3.5-turbo (fast, cheap)
#   - custom-enterprise-model (if available to your organization)
#
# To use: CONTEXT_GENERATOR_MODEL=zai/gpt-4-turbo
#
# Features:
# - Enterprise API endpoint for custom deployments
# - Self-hosted or managed service options
# - Custom model fine-tuning support
# - Dedicated account management

# -----------------------------------------------------------------------------
# n8n Integration
# -----------------------------------------------------------------------------
# https://n8n.io
# For workflow automation and webhook triggers

# n8n instance URL (self-hosted or cloud)
# N8N_HOST=https://n8n.homelab.martinmayday.com
# N8N_HOST=https://your-instance.app.n8n.cloud
#N8N_HOST=https://n8n.example.com

# Webhook authentication header and token
# Generate a secure token in n8n: Settings → Webhooks
#N8N_WEBHOOK_AUTH_HEADER=X-N8N-Auth-Token
#N8N_WEBHOOK_TOKEN=your-secure-webhook-token-here

# Webhook URLs for context generator events
# Set up these webhooks in n8n to receive completion notifications
#N8N_WEBHOOK_ON_COMPLETE=https://n8n.homelab.martinmayday.com/webhook/context-generator/complete
#N8N_WEBHOOK_ON_ERROR=https://n8n.homelab.martinmayday.com/webhook/context-generator/error

# n8n API Key for workflow triggering (optional)
# Get from: Settings → API → Create API Key
#N8N_API_KEY=your-n8n-api-key-here

# -----------------------------------------------------------------------------
# Advanced Configuration
# -----------------------------------------------------------------------------

# Request timeout in seconds
LLM_TIMEOUT=60

# Max retries on failure
LLM_MAX_RETRIES=3

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.3

# Max tokens for responses
LLM_MAX_TOKENS=500

# Enable debug logging
DEBUG=false

# Cache directory for LLM responses (reduces API costs)
CACHE_DIR=~/.cache/context-generator

# -----------------------------------------------------------------------------
# Proxy Configuration (if behind corporate firewall)
# -----------------------------------------------------------------------------

# HTTP_PROXY=http://proxy.company.com:8080
# HTTPS_PROXY=http://proxy.company.com:8080
# NO_PROXY=localhost,127.0.0.1,.local
