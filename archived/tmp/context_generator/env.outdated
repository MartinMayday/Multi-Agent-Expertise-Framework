# =============================================================================
# Context Generator - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# Never commit .env to version control
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Selection
# -----------------------------------------------------------------------------
# Format: provider/model
# Examples:
#   - ollama/llama3.2
#   - lmstudio/local-model
#   - openrouter/anthropic/claude-3-haiku
#   - gemini/gemini-1.5-flash
#   - moonshot/moonshot-v1-8k
#   - offline (no LLM, static analysis only)

CONTEXT_GENERATOR_MODEL=offline

# -----------------------------------------------------------------------------
# Ollama (Local)
# -----------------------------------------------------------------------------
# https://ollama.ai
# Default: http://localhost:11434

OLLAMA_HOST=http://localhost:11434

# Available models (run `ollama list` to see installed):
#   - llama3.2
#   - llama3.2:3b
#   - codellama:7b
#   - mistral
#   - deepseek-coder:6.7b
#   - qwen2.5-coder:7b

# -----------------------------------------------------------------------------
# LM Studio (Local)
# -----------------------------------------------------------------------------
# https://lmstudio.ai
# Default server runs on port 1234, OpenAI-compatible API

LMSTUDIO_HOST=http://localhost:1234
LMSTUDIO_API_KEY=lm-studio

# LM Studio uses OpenAI-compatible endpoint:
# POST http://localhost:1234/v1/chat/completions

# -----------------------------------------------------------------------------
# OpenRouter (Cloud - Multi-provider)
# -----------------------------------------------------------------------------
# https://openrouter.ai
# Get API key: https://openrouter.ai/keys

OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Popular models:
#   - anthropic/claude-3-haiku (fast, cheap)
#   - anthropic/claude-3.5-sonnet (balanced)
#   - google/gemini-flash-1.5 (fast)
#   - deepseek/deepseek-chat (cheap, good for code)
#   - meta-llama/llama-3.1-70b-instruct (open source)

# Optional: Site info for OpenRouter analytics
OPENROUTER_SITE_URL=https://github.com/context-generator
OPENROUTER_SITE_NAME=Context Generator

# -----------------------------------------------------------------------------
# Google Gemini (Cloud)
# -----------------------------------------------------------------------------
# https://ai.google.dev
# Get API key: https://aistudio.google.com/apikey

GEMINI_API_KEY=AIzaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Models:
#   - gemini-1.5-flash (fast, cheap, 1M context)
#   - gemini-1.5-flash-8b (fastest, cheapest)
#   - gemini-1.5-pro (best quality, 2M context)
#   - gemini-2.0-flash-exp (experimental)

# Optional: Custom endpoint
GEMINI_API_BASE=https://generativelanguage.googleapis.com/v1beta

# -----------------------------------------------------------------------------
# Moonshot AI (Cloud - China)
# -----------------------------------------------------------------------------
# https://www.moonshot.cn / https://platform.moonshot.cn
# Get API key: https://platform.moonshot.cn/console/api-keys

MOONSHOT_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Models:
#   - moonshot-v1-8k (8K context)
#   - moonshot-v1-32k (32K context)
#   - moonshot-v1-128k (128K context)

MOONSHOT_API_BASE=https://api.moonshot.cn/v1

# -----------------------------------------------------------------------------
# ZAI Codingplan (Custom/Enterprise)
# -----------------------------------------------------------------------------
# Configure your ZAI Codingplan endpoint

ZAI_API_KEY=zai-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
ZAI_API_BASE=https://api.zai.codingplan.io/v1
ZAI_MODEL=codingplan-v1

# -----------------------------------------------------------------------------
# n8n Integration
# -----------------------------------------------------------------------------
# https://n8n.io
# For workflow automation and webhook triggers

# n8n instance URL (self-hosted or cloud)
N8N_HOST=https://n8n.homelab.martinmayday.com
# N8N_HOST=https://your-instance.app.n8n.cloud

# Webhook authentication
N8N_WEBHOOK_AUTH_HEADER=X-N8N-Webhook-Token
N8N_WEBHOOK_TOKEN=your-secure-webhook-token-here

# Webhook endpoints for context generator events
N8N_WEBHOOK_ON_COMPLETE=https://n8n.homelab.martinmayday.com/webhook/context-generator/complete
N8N_WEBHOOK_ON_ERROR=https://n8n.homelab.martinmayday.com/webhook/context-generator/error

# Optional: n8n API for workflow triggering
N8N_API_KEY=n8n_api_xxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Advanced Configuration
# -----------------------------------------------------------------------------

# Request timeout in seconds
LLM_TIMEOUT=60

# Max retries on failure
LLM_MAX_RETRIES=3

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.3

# Max tokens for responses
LLM_MAX_TOKENS=500

# Enable debug logging
DEBUG=false

# Cache directory for LLM responses (reduces API costs)
CACHE_DIR=~/.cache/context-generator

# -----------------------------------------------------------------------------
# Proxy Configuration (if behind corporate firewall)
# -----------------------------------------------------------------------------

# HTTP_PROXY=http://proxy.company.com:8080
# HTTPS_PROXY=http://proxy.company.com:8080
# NO_PROXY=localhost,127.0.0.1,.local
